---
title: "How to use ollama for windows(beta) from WSL"
emoji: "ğŸ¤–"
type: "idea" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["ollama","llm"]
published: false
---

ä½ã‚¹ãƒšãƒƒã‚¯ç¸›ã‚Šã§LLMã‚’ä½¿ãˆã‚‹ã®ã‹ï¼Ÿã¨ã„ã†å®Ÿé¨“ã€‚
å…·ä½“çš„ã«ã¯GitHub Copilotã¨åŒç­‰ã®ã‚„ã‚Šå–ã‚ŠãŒã§ãã‚‹ã‹ã€‚

## çµè«–

ç„¡ç†ã€‚å¤§äººã—ãé«˜ã‚¹ãƒšãƒƒã‚¯ã®ãƒã‚·ãƒ³ã‚’ç”¨æ„ã—ã‚ˆã†ã€‚
ã‚ã‚‹ã„ã¯GitHub Copilotã‚„ChatGPTãŒä½¿ãˆã‚‹ã‚ˆã†æ ¹å›ã—ã—ã‚ˆã†ã€‚
~~æœ€çµ‚æ‰‹æ®µ: è»¢~~

## æ¤œè¨¼ç’°å¢ƒ
- ãƒ›ã‚¹ãƒˆ
	- OS: Windows 10 Pro
    - CPU: Intel Core i5-9400F
    - RAM: 8.00 GB
    - DISK: 1TB HDD
    - GPU: GTX 1050
- WSL
	- OS: Ubuntu 22.04
    - RAM: 3.00GB
    - SWAP: 8.00GB

## LLMã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
æ¤œè¨¼ç’°å¢ƒã«ã‚‚ã‚ã‚‹ã¨ãŠã‚Šæ‚²ã—ã„ã»ã©ã®ä½ã‚¹ãƒšãƒƒã‚¯ã€‚
WSLã‚‚3GBã§ã¯ã¾ã¨ã‚‚ãªLLMã‚’ä½¿ãˆãªã„ã€‚
ã ãŒã€WSLã‹ã‚‰Windowsã®ollamaã‚’å‘¼ã¹ã°ãã‚Œãªã‚Šã®LLMã‚’ä½¿ãˆã‚‹ã®ã§ã¯ãªã„ã‹ã¨æ€ã£ãŸã€‚
ãã‚Œã§ã‚‚8GBã—ã‹ãªã„ãŒãƒ»ãƒ»ãƒ»

https://github.com/ollama/ollama?tab=readme-ov-file#model-library ã«ã¯
> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.

ã¨æ›¸ã„ã¦ã‚ã‚‹ã€‚Bã£ã¦ãªã‚“ã®ã“ã£ã¡ã‚ƒï¼Ÿãƒã‚¤ãƒˆï¼Ÿãƒ“ãƒªã‚ªãƒ³ï¼Ÿ
8GB RAMã ã¨7Bã§ã‚‚ã‚®ãƒªã‚®ãƒªã ãŒã€ã“ã‚Œã¯7Bæœªæº€ãªã‚‰å¤šåˆ†å¯èƒ½ã€ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚

æµçŸ³ã«7Bæœªæº€ã§GitHub Copilotã¨åŒç­‰ã®ä½“é¨“ã‚’å¾—ã‚‹ã“ã¨ã¯é›£ã—ã„ã ã‚ã†ãŒã€ã‚„ã£ã¦ã¿ãªã„ã¨ã‚ã‹ã‚‰ãªã„ã€‚

- [gemma](https://ollama.com/library/gemma)
	- param
		- 2B
		- 7B
	- LICENSE
		- ä¸€èˆ¬çš„ãªã‚„ã¤ã§ã¯ãªã•ãã†
- [mistral](https://ollama.com/library/mistral)
	- param
		- 7B
	- LICENSE
		- Apache
- [codellama](https://ollama.com/library/codellama)
	- param
		- 7B
	- LICENSE
		- ä¸€èˆ¬çš„ãªã‚„ã¤ã§ã¯ãªã•ãã†
- [llama3.2](https://ollama.com/library/llama3.2)
	- param
		- 1B
		- 3B
	- LICENSE
		- ä¸€èˆ¬çš„ãªã‚„ã¤ã§ã¯ãªã•ãã†

ã“ã“ã‚‰ã¸ã‚“ãŒæ°—ã«ãªã‚‹ã®ã§ä¸€é€šã‚Šå‹•ã‹ã—ã¦ã¿ã‚‹ã€‚

```powershell
ollama pull llama3.2:3b
ollama pull llama3.2:1b
ollama pull codellama:7b
ollama pull mistral:7b
ollama pull gemma:2b
ollama pull gemma:7b
```

pueueã‚’ä½¿ã†å ´åˆ

```powershell
pueue add -- "ollama pull llama3.2:3b"
pueue add -- "ollama pull llama3.2:1b"
pueue add -- "ollama pull codellama:7b"
pueue add -- "ollama pull mistral:7b"
pueue add -- "ollama pull gemma:2b"
pueue add -- "ollama pull gemma:7b"
```

## LLMã¨ã®ã‚„ã‚Šå–ã‚Š

APIã‚’å©ãã“ã¨ã§å¯èƒ½ã€‚

ã“ã“ã§ã¯ `llama3.2:3b` ã‚’ä½¿ã†ã€‚

```powershell
ollama run llama3.2:3b

(Invoke-WebRequest -method POST -Body '{"model":"llama3.2:3b", "prompt":"Why is the sky blue?", "stream": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
```

ãŸã éƒ½åº¦éƒ½åº¦APIã‚’å©ãã®ã‚‚é¢å€’ã ã—ã€ä½¿ã£ã¦ã‚‹æ„Ÿã˜ãŒã—ãªã„ã®ã§ã€Neovimãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½¿ã†ã€‚

[codecompanion.nvim](https://github.com/olimorris/codecompanion.nvim) ã‚’ä½¿ã†ã€‚

è‡ªåˆ†ã®è¨­å®šã¯ä»¥ä¸‹å‚ç…§ã€‚

https://github.com/mimikun/dotfiles/blob/master/dot_config/nvim/lua/plugins/codecompanion-nvim.lua
https://github.com/mimikun/dotfiles/blob/master/dot_config/nvim/lua/plugins/configs/codecompanion-nvim/cmds.lua
https://github.com/mimikun/dotfiles/blob/master/dot_config/nvim/lua/plugins/configs/codecompanion-nvim/dependencies.lua
https://github.com/mimikun/dotfiles/blob/master/dot_config/nvim/lua/plugins/configs/codecompanion-nvim/keys.lua
https://github.com/mimikun/dotfiles/blob/master/dot_config/nvim/lua/plugins/configs/codecompanion-nvim/opts.lua

é‡è¦ãªã®ã¯ã“ã®éƒ¨åˆ†ã€‚

https://github.com/mimikun/dotfiles/blob/a114d278337da7ff95675f11f3a471b66d60658f/dot_config/nvim/lua/plugins/configs/codecompanion-nvim/opts.lua#L10-L24

ã“ã®è¨­å®šã‚’ã—ã¦Neovimã§ `:CodeCompanionChat` ã¨æ‰“ã¤ã€‚

